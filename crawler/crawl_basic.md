# 爬虫
### 爬虫设计思路
1. 首先确定需要爬取的网页url，放入待爬取队列
2. 通过http协议获取对应的HTML页面，把爬过的放入已爬取队列
3. 提取页面里有用的数据
   
### 可以做爬虫的语言
1. php：但是对多线程、异步支持不够好
2. java：Java语言本身比较笨重，代码量很大，重构成本高
3. c/c++：运行效率高，但是学习成本高
4. python：代码简洁，支持库多

### 相关技术
1. http请求的处理：urllib\urllib2(为python内置库)，requests
2. 解析响应页面的内容：
   * 文本解析：re、xpath、beautifulsoup4（bs4）、jsonpath、pyquery等
3. 如何动态加载html、验证码：
   * Selenium+PhantomJS（无界面、内存里面跑）
   * Tesseract：机器学习库，机器图像识别系统
   * 打码平台【由人工验证的】
4. Scrapy框架【还有同类型是：Pyspider】
   * 高定制性、高性能（异步框架：twisted），所以下载数据速度快
   * 提供数据存储、数据下载、提取规则等组件
5. 分布式策略
   * scrapy-redis，在scrapy的基础上添加了以Redis数据库为核心一套组件，让scrapy框架支持分布式功能；
     * 主要用于：请求指纹去重（不要让其他爬虫采取已经采集的页面）、请求分配、数据临时存储
6. 爬虫与反爬虫     
   * 反爬虫技术：user-agent、代理、验证码、动态数据加载、加密数据（一般也是在某个js里面有解密机制，都否正常浏览器也无法解析、）      
   
### robots协议
* 也称为爬虫协议、机器人协议等，全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。
* 这个协议也不是一个规范，而只是约定俗成的，有些搜索引擎会遵守这一规范，而其他则不然。通常搜索引擎会识别这个元数据，不索引这个页面，以及这个页面的链出页面。

### 注意事项
* 发送请求一定要带上User-Agent，伪装成浏览器的样子，否则会被服务器是爬虫发现封ip
* 最好不要写Accept-Encoding: gzip, deflate, br，因为指定gzip格式返回还需要自己解压处理
* 我们在请求的时候，输入中文需要URLEncode转码，当字符串数据以url的形式传递给web服务器时，字符串中是不允许出现空格和特殊字符串的
  * http://web.chacuo.net/charseturlencode
* post需要用抓包工具看请求体的内容  
* 如果是ajax动态页面，通常是通过抓包看实际请求数据的url，而非我们所看到的页面url地址

### 代理网站
* 西刺： https://www.xicidaili.com/nn/
* 快代理（含付费，公司用）： https://www.kuaidaili.com/

### 模拟登陆
* 登录一般都会先有一个HTTP GET，用于拉取一些信息及获得Cookie，然后再HTTP POST登录。
* HTTP POST登录的链接有可能是动态的，从GET返回的信息中获取。
* password 有些是明文发送，有些是加密后发送。有些网站甚至采用动态加密的，同时包括了很多其他数据的加密信息，只能通过查看JS源码获得加密算法，再去破解加密，非常困难。
* 有的反爬虫会监测鼠标活动的特征，准确地判断出是不是真人在操作，所以Selenium+PhantomJS