# 软件测试基础
### 软件测试概念
* 软件测试是一个过程，包含若干活动
* 软件测试的目的不仅仅是为了发现错误

### 为何要软件测试
* 一个糟糕的测试程序可能导致任务的失败，影响操作的性能和可靠性，导致维护阶段的成本提高。
* 一个好的测试程序是项目的主要成本。
* 一个好的测试程序可以极大地帮助你定义需求和设计。
* 一个好的测试可以迫使你在工作时必须面对和处理问题，使得修改缺陷成本降低。
* 一个好的测试不能弥补一个糟糕的软件项目，但是的确有助于发现许多问题，并且至少使得你尽早知道你处在问题当中。

### 软件缺陷与bug
* 软件缺陷：既指静态存在于软件工作产品（文档、代码）中的错误，也指软件运行时由于这些错误被激发引起的和软件产品预期属性的偏离现象。
* Bug：代码中的缺陷。有时也被泛指因软件产品内部的缺陷引起的软件产品最终运行时和预期属性的偏离。
* 实际工作中，可以把软件缺陷认为是bug
* 缺陷分类：
  1. 遗漏
     * 规定的或预期的需求未体现在产品中（可能未将规格说明全面实现，也可能需求分析阶段就遗漏了需求）
  2. 错误
     * 未将规格说明正确实现（可能设计错误、也可能编码错误）
  3. 额外的实现
     * 规格说明并未规定的需求被纳入产品，得到实现
* 满足下面5个规则中的一个就认为发生了一个缺陷：
  1. 软件未实现产品说明书SRS要求的功能
  2. 软件出现了SRS指明不该出现的错误
  3. 软件实现了SRS未提及的功能
  4. 软件未实现SRS虽未明确提及但应该实现的功能
  5. 软件难以理解、不易使用、运行缓慢或从测试人员角度看，最终用户认会为不好。
### 测试分类
* 软件测试方法划分：
  * 黑盒测试
    * 把被测对象看成一个黑盒，只考虑其整体特性，不考虑其内部具体实现。
    * 被测对象可以是一个系统、一个子系统、一个模块、一个子模块、一个函数等。
    * 常用的黑盒测试方法（即测试用例的设计方法）：
      * 等价值划分类
      * 边界值分析法
      * 因果图分析法
      * 判定表
      * 状态迁移法
    * 以上不管是什么测试方法，都是为了减少测试时的测试用例数，都是为了用尽量少的测试用例去完成测试，去发现更多的问题
  * 白盒测试
    * 白盒测试是依据被测软件分析程序内部构造，并根据内部构造设计用例，来对内部控制流程进行测试，可完全不顾程序的整体功能实现情况。
    * 白盒测试一般在测试前期进行，通过达到一定的逻辑覆盖率指标，使得软件内部逻辑控制结构上的问题能基本得到消除
    * 白盒测试发现问题后解决问题的成本较低
    * 白盒常用方法：
      * 静态分析：控制流分析、数据流分析、信息流分析等
      * 动态分析：逻辑覆盖测试（分支测试、路径测试等）、程序插桩等
  * 灰盒测试
* 软件测试种类：
  * 人工测试
  * 自动化测试
    * 基于人工测试之上
    * 对程序新版本运行前一版本执行的测试，提高**回归测试**效率，可以更快地将软件推向市场
    * 可以运用于更多更频繁的测试，比如**冒烟测试**
    * 可以执行手工测试困难或不可能做的测试，比如大量的重复操作或者集成测试
    * 不能取代手工测试，自动化测试只能提高测试效率，**不能提高测试有效性**，即不可能发现更多缺陷
### 测试阶段划分
* 单元测试（Unit Testing）-UT
  * 单元测试属于白盒测试
  * 主要测试单元内部的数据结构、逻辑控制、异常处理等
  * 单元测试的评估基准主要是**逻辑覆盖率**
  * 可能需要编写驱动模块或者桩模块
    * 驱动模块：模拟被测模块的上一级模块（调用被测模块的那个模块）
    * 桩模块：模拟被测模块的下一级模块（被测模块调用的那个模块）

* 集成测试（Integration Testing）-IT
  * 集成测试属于灰盒测试范畴
  * 主要测试模块之间的接口和接口数据传递关系，以及模块组合后的整体功能
  * 集成测试的评估基准主要是**接口覆盖率**已实现接口数/总的接口数（实际结果）计划实现接口数/总的接口数（预期结果）
  * 系统测试的评估基准主要是**测试用例对需求规格的覆盖率** — 需求跟踪矩阵表
  * 冒烟测试也叫版本验证测试，提交测试
    1. 拿到一个新的集成版本的时候，一般先做冒烟测试。
       * 冒烟测试：利用较少的时间（0.5天-2天）、较少的人（1-3人，经验更丰富）对软件的主要功能进行测试，主要就是判断该版本是否值得一测。如果值得一测，那么整个测试组再全部投入；如果不值得一测，打回开发组，另其完善后再给出新版本。
    2. 拿到一个新版本测试思路：
       1. 冒烟测试
       2. 返测：对于发现的缺陷是否进行修复的测试。
       3. 回归测试：对前面版本中所有用例在执行一遍（保证软件旧的功能正确）
       4. 对新添加的功能进行测试。

* 系统测试（System Testing）-ST
  * 系统测试属于黑盒测试范畴
  * 主要测试整个系统相对于软件需求规格说明书SRS的符合度
  * 对整个软件系统进行全面完整的测试过程。
  * 在系统测试之前一般有“确认测试”【一个大的冒烟测试】
    * 确认该程序值不值得一测（冒烟测试）
    * 确认相关文档是否齐全（尤其是交给用户的文档）

* 验收测试 (User Accept Testing)-UAT【以用户为主】
  * α（ALPHA)测试（用户在开发环境下进行，或者由开发内部的用户【产品经理】在模拟实际操作环境下进行的测试）
  * β（BETA)测试（用户在生产环境下验收，开发者通常不在测试现场）

### 回归测试
* 软件在测试或其他活动中发现的缺陷经过修改后，应该进行回归测试（Regression Testing）。
* 目的是验证缺陷得到了正确的修复，同时对系统的变更没有影响以前的功能
* **回归测试可以发生在任何一个阶段**，包括单元测试、集成测试和系统测试
* 回归测试流程
  1. 在测试策略制定阶段，制定回归测试策略
  2. 确定需要回归测试的版本 Version 1.2.3000(发现缺陷的版本)1.2.3003
  3. 回归测试版本发布,按照回归测试策略执行回归测试
  4. 回归测试通过，关闭缺陷跟踪单（问题单）
  5. 回归测试不通过，缺陷跟踪单返回开发人员，开发人员重新修改问题，
再次提交测试人员回归测试
* 回归测试策略
  1. 完全重复测试
  2. 选择性重复测试
     * 覆盖修改法
       * 即针对被修改的部分，选取或重新构造测试用例验证没有错误再次发生
     * 周边影响法
       * 该方法不但要包含覆盖修改法确定的用例，还需要分析修改的扩散影响，对那些受到修改间接影响的部分选择测试用例验证它没有受到不良影响。该方法比覆盖修改法更充分一点。
     * 指标达成法
       * 这是一种类似于单元测试的方法，在重新执行测试前，先确定一个要达成的指标，如修改部分代码100%的覆盖、与修改有关的接口60%的覆盖等，基于这种要求选择一个最小的测试用例集合。

### 测试过程阶段划分
* 测试**计划**阶段
  * 输入：输入物：项目计划、开发文档（SRS、SLD、LLD等）
  * 输出：需求分析、测试计划，需审核
* 测试**设计**阶段
  * 输入：上阶段的输出
  * 输出：测试方案，需审核
* 测试**实现**阶段
  * 输入：上阶段的输出
  * 输出：测试用例、测试规程，需审核
* 测试**执行**阶段
  * 输入：上阶段的输出
  * 输出：测试缺陷、测试报告

### 常见的测试过程模型
* V模型
  * 仅仅把测试过程作为在需求分析、系统设计及编码之后的一个阶段
  * 忽视了测试对需求分析,系统设计的验证，一直到后期的验收测试才被发现。
  * ![img](https://github.com/ericyishi/img-folder/blob/master/summary/testing/vModel.gif)
* V&V模型（W模型）
  * 测试与开发是同步进行的（双v）
  * 测试的对象不仅仅是程序，还包括需求和设计
  * W模型有利于尽早地全面的发现问题
  * 测试和开发活动也保持着一种线性的前后关系，上一阶段完全结束，才可正式开始下一个阶段工作。这样就无法支持迭代的开发模型
  * ![img](https://github.com/ericyishi/img-folder/blob/master/summary/testing/wModel.jpg)
* H模型
  * H模型将测试活动从开发流程完全独立出来，使测试流程形成一个完全独立的流程，将测试准备活动与测试执行活动清晰地体现出来。
  * 软件测试原则“尽早准备，尽早执行”；强调测试是独立的，只要测试准备完成，就可以执行测试。
  * 本模型太过于模型化，重点在于理解其中的意义指导实际工作，而模型本身并无太多的可执行的指导意义
* X模型
  * 探索性测试，能够帮助有经验的测试人员在测试计划之外发现更多的软件错误
* 敏捷测试模型
  * 强调从客户角度进行测试
  * 重点关注迭代测试的新功能，不再强调测试阶段。
  * 尽早测试，不间断测试，具备条件即测试
  * 强调持续反馈
  * 预防缺陷终于发现缺陷

### 敏捷测试与传统测试
<table>
   <thead>
    <tr>
      <th>传统测试</th>
      <th>敏捷测试</th>
    </tr>
   </thead>
   <tbody>
   <tr>
       <td>测试是质量的最后保护者</td>
       <td>开发和测试人员紧密合作，大家都有责任对软件负责</td>
   </tr>
   <tr>
       <td>严格的变更管理</td>
       <td>变更是可以接受的，拥抱变更</td>
   </tr>
   <tr>
       <td>预先计划和细节的准备</td>
       <td>计划随着进展时长调整</td>
   </tr>
   <tr>
       <td>重量级文档</td>
       <td>只需要必要文档</td>

   </tr>
   <tr>
       <td>各阶段测试严格的入口和出口标准</td>
       <td>各迭代之间没有明显的入口和出口标准</td>

   </tr>
    <tr>
       <td>更多在回归测试时进行重量级的自动化测试</td>
       <td>所有阶段都需要自动化测试，每个人都需要做，是项目集成的一部分</td>
   </tr>
   <tr>
       <td>严格依赖流程执行</td>
       <td>流程不再需要严格执行</td>
   </tr>
      <tr>
          <td>测试团队和开发团队是相对独立的</td>
          <td>团队合作是无间隙合作</td>
      </tr>
   </tbody>
  </table>

### 自动化测试应用场景
1. 产品需求变更较少
2. 项目开发周期较长
3. 测试用例执行频繁
4. 手工测试无法胜任
   * 7*24小时的收集资源信息
5. 人物财力资源充足   

### web自动化与移动端自动化的区别
1. 执行用例的差别：
   * app端：一部手机只能让一个应用在主屏幕上，其他apk会在后台继续运行
   * web端：可以通过python开启多线程，控制多个浏览器测试不同的测试用例
   
2. 是否安装
   * app端：这点感觉像C/S架构风格，因为app是需要安装的，同时安装卸载也是一个测试检查点
   * web端：无需安装，只需有浏览器即可访问。
   
3. 不可见元素的操作
   * app端：只会显示在手机页面里加载出来的部分，比方说有的页面比较长，需要向下滑动才能看到更多信息，此时需要保证不可见的元素显示在手机页面才能对它操作。
   * web端：①如果控件不是下拉产生的异步加载，那么我们是可以对屏幕内不可见的元素做操作的。因为DOM节点已经生成②对于是如果遇到下拉才能够加载，可以使用js操作滚动条。
   
4. 元素定位：
   * app端：部分定位方式不支持，比如css_selector以及link_text
   * web端：基础就name,id,class_name,css,xpath
5. 启动
   * app端：需要定制desired_caps内容
   * web端：通过启动webdriver不同的浏览器类来获取driver
6. js的使用
   * app端：支持不太好
   * web端：支持好
7. 关于滑动
   * app端：运用地很多
   * web端：用到得不多